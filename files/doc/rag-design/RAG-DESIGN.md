# 提高召回质量
## 1. 父子文档设计
涉及代码：
- zilliz_small_big_chunk_demo.py
- text_input_handler.split_text
- rag_service.add_articles_to_vector_store
- article_routes.batch_vector_store

### 1.1 整体流程
父子文档设计的目的是在保证召回质量的同时，兼顾上下文的完整性。整个流程分为三个阶段：

#### (1) 文档切分阶段 (TextInputHandler.split_text)
- 输入：原始文本
- 处理流程：
  1. 使用JINA等语义切分工具进行初步切分，得到句子语义级别的小段落(nodes)，每个node长度不等，可能在数十到百字的范围
  2. 基于最大长度(max_chunk_length 默认1000)和重叠长度(chunk_overlap 默认100)参数，将相邻的小段落合并成大段落(big_chunks)
  3. 维护小段落到大段落的映射关系(small_big_dict)
- 输出：
  - big_chunks：合并后的大段落列表
  - nodes：原始的小段落列表
  - small_big_dict：小段落ID到大段落ID的映射字典

#### (2) 存储阶段 (rag_service.add_articles_to_vector_store)
采用"双存储"策略：
1. 向量数据库存储：
   - 存储对象：小段落(nodes)
   - 存储内容：
     - 文本内容
     - 向量嵌入(1024维度)
     - 元数据(文章ID、标题、chunk_id等)
   - 目的：用于语义相似度检索

2. Redis缓存存储：
   - 存储对象：大段落(big_chunks)
   - 存储格式：Hash结构 {article_chunks: {article_1_chunk_19: "..."}}
   - 目的：保存完整上下文

#### (3) 检索应用阶段
1. 首次检索：
   - 通过向量数据库检索相似的小段落(nodes)
   - 获取检索结果的chunk_id

2. 上下文扩展：
   - 根据chunk_id从Redis中获取对应的大段落
   - 返回完整的上下文内容给LLM

### 1.2 优势
1. 检索精准性：小段落粒度的向量检索确保了相关内容的准确匹配
2. 上下文完整性：通过大段落的缓存保证了上下文的连贯性
3. 存储效率：向量库只存储必要的小段落，减少了向量计算和存储开销
4. 快速访问：使用Redis缓存大段落，保证了上下文获取的高效性

### 1.3 关键配置
- CHUNK_SIZE：大段落最大长度(默认1000)
- OVERLAP：段落重叠长度(默认100)
- max_node_length：小段落最大长度(CHUNK_SIZE - OVERLAP)

### 1.4 总结
父子文档设计通过将文档切分为小段落和大段落，分别存储在向量数据库和Redis缓存中，实现了高效的检索和上下文获取。这种设计不仅提高了召回质量，还兼顾了上下文的完整性和存储效率。

## 2. 上下文设计

### 2.1 问题背景
传统RAG系统在文档切分时往往会丢失上下文信息。例如，当一个文档块包含"该公司收入增长3%"这样的信息时，如果没有上下文，我们无法知道具体是哪家公司、哪个时期的数据。

### 2.2 上下文召回方案
基于Anthropic的Contextual Retrieval方案，我们采用以下设计：

#### (1) 上下文生成
- 输入：原始文档块
- 处理：使用LLM为每个文档块生成简洁的上下文说明（50-100 tokens）
- 输出：上下文 + 原文档块的组合

#### (2) 双重检索策略
1. 上下文嵌入(Contextual Embeddings)：
   - 在生成文档块的向量表示前，先添加上下文信息
   - 使上下文信息参与语义向量的生成
   - 提高语义检索的准确性

2. 上下文BM25(Contextual BM25)：
   - 将上下文信息加入BM25索引
   - 增强对特定术语、标识符的精确匹配能力
   - 补充语义检索的不足

#### (3) 混合排序
- 结合向量相似度和BM25分数
- 使用排序融合技术(Rank Fusion)合并两种检索结果
- 去重并选择Top-K个最相关的文档块

### 2.3 实现要点
1. 上下文生成：
   - 使用LLM生成上下文，确保信息简洁、相关
   - 缓存生成的上下文，避免重复计算
   - 控制上下文长度，建议50-100 tokens

2. 检索优化：
   - 合理设置chunk大小和重叠度
   - 选择合适的embedding模型
   - 调整BM25参数以平衡精确匹配和语义匹配

3. 性能考虑：
   - 使用缓存减少LLM调用成本
   - 批量处理文档块的上下文生成
   - 优化向量检索的性能

### 2.4 优势
1. 提高检索准确性：通过上下文信息提升检索质量
2. 增强语义理解：帮助模型更好地理解文档块的含义
3. 改善用户体验：提供更准确、更相关的答案
4. 成本可控：通过缓存机制控制LLM调用成本

### 2.5 注意事项
1. 上下文长度：需要平衡信息完整性和token开销
2. 检索策略：根据具体场景调整向量检索和BM25的权重
3. 性能优化：大规模数据时需要注意检索性能
4. 成本控制：合理使用缓存机制降低运营成本

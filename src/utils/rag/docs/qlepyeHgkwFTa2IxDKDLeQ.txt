Title: Scaling Test-Time Compute：向量模型上的思维链

URL Source: https://mp.weixin.qq.com/s/qlepyeHgkwFTa2IxDKDLeQ

Markdown Content:
自从 OpenAI 发布了 o1 模型后，**Scaling Test-Time Compute**（扩展推理时计算）就成了 AI 圈子里最火爆的话题之一。简单来说，与其在预训练或后训练阶段疯狂堆算力，不如在推理阶段（也就是大语言模型生成输出的时候）多花点计算资源。

o1 模型将一个大问题拆分为一系列小问题（即思维链，Chain-of-Thought），让模型像人一样一步步思考，评估不同的可能性、做更细致的规划，给出答案前进行自我反思等。通过这种方式，模型无需重新训练，仅通过推理时的额外计算就能提高性能。

**与其让模型死记硬背，不如让它多思考**—— 这种策略在复杂的推理任务中尤为有效，效果提升显著，阿里巴巴最近发的 QwQ 模型也印证了这一技术趋势：通过拓展推理时计算来提升模型能力。

👩‍🏫 本文的 Scaling 指的是在推理过程中增加计算资源（例如算力或时间）。它不是指横向扩展（分布式计算）或加速处理（缩短计算时间）。

![Image 21](https://mmbiz.qpic.cn/sz_mmbiz_gif/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geialessQjYesFytnaYtcDm0YZ8HXqia7p21EGkZ6uuLCXSLtvg28BWibgVA/640?wx_fmt=gif&from=appmsg)

你要是也用过 o1 模型，肯定会感觉到多步推理更费时，因为模型需要构建思维链来解决问题。

在 Jina AI，相比大型语言模型（LLMs），我们更专注于 Embeddings 和 Rerankers。因此，我们自然就想到了：**能不能把“思维链”的概念也应用到 Embedding 模型上？**

虽然乍一看可能不太直观，但本文将探讨一种新的视角，并演示如何把扩展推理时计算（Scaling Test-Time Compute）应用到`jina-clip`，以对 **棘手的领域外（Out Of Domain, OOD）图像** 进行分类，来解决原本不可能的任务。

![Image 22](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geiaAsJ7DAoPdq07AdB3TjpEWg6Mt9dBxhEA5jdeJOeDKRiaQLibibJXuZoiaQ/640?wx_fmt=png&from=appmsg)

我们拿宝可梦的识别来做实验，这对向量模型来说还是挺有挑战性的。CLIP 这种模型虽然在图像-文本匹配上很强，但碰到模型没见过的、领域外（OOD）的数据就容易翻车。

然而，我们发现，**通过增加模型推理时间，采用类似于思维链的多目标分类策略，不需要调整模型，也能提高领域外数据的分类准确率。**

### 案例研究：宝可梦图像分类

🔗 Google Colab: https://colab.research.google.com/drive/1zP6FZRm2mN1pf7PsID-EtGDc5gP_hm4Z#scrollTo=CJt5zwA9E2jB

我们使用了 TheFusion21/PokemonCards 数据集，里面有几千张宝可梦卡牌图像。**这是一个图像分类任务**，输入一张裁剪过的宝可梦卡牌（去掉了文字描述），输出正确的宝可梦名字。但这对 CLIP Embedding 模型来说是个难题，原因有几个：

*   宝可梦的名字和样子对模型来说都比较陌生，直接分类很容易翻车。
    
*   **每只宝可梦都有自己的视觉特点**，比如形状、颜色、姿势，这些 CLIP 比较好理解。
    
*   卡牌的风格虽然统一，**但不同的背景、姿势和画风又增加了难度**。
    
*   **这个任务需要****同时考虑多个视觉特征**，就像 LLM 里的复杂思维链。
    

![Image 23](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geia2O1C7OtQekyAZOQdzrHqFnrFKgltzxHMUZdFWFRCtIWTjicn1ibibg1Tg/640?wx_fmt=png&from=appmsg)

我们把卡牌上的文字信息（标题、页脚、描述）都去掉了，免得模型作弊，直接从文字里找到答案，因为这些宝可梦类别的标签就是它们的名字，比如 Absol、Aerodactyl。

#### 基准方法：直接相似度比较

先说说最简单的基准方法 (Baseline)，就是 **直接比较宝可梦图片和名字的相似度**。

首先，还是把卡牌上所有的文字信息都去掉，免得 CLIP 模型直接通过文本来猜测答案。然后，我们用 `jina-clip-v1` 和 `jina-clip-v2` 模型分别对图片和宝可梦名字进行编码，得到它们各自的向量表示。最后，计算图像向量和文本向量之间的余弦相似度，哪个名字的相似度最高，就认为图片是哪个宝可梦。

这种方法相当于在图片和名字之间做了一对一的匹配，没考虑其他的上下文信息或属性。下面这段伪代码简单描述了这个过程。

```
# 预处理cropped_images = [crop_artwork(img) for img in pokemon_cards]  # 去掉文字，只保留图片pokemon_names = ["Absol", "Aerodactyl", ...]  # 宝可梦名字# 用 jina-clip-v1 获取 embeddingsimage_embeddings = model.encode_image(cropped_images)text_embeddings = model.encode_text(pokemon_names)# 计算余弦相似度进行分类similarities = cosine_similarity(image_embeddings, text_embeddings)predicted_names = [pokemon_names[argmax(sim)] for sim in similarities]  # 哪个名字相似度最高，就选哪个# 评估准确率accuracy = mean(predicted_names == ground_truth_names)
```

#### 进阶：把思维链应用到图像分类

![Image 24](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geia6u420YbE3zafxfJAtc5nXblqrpru1BNgicrtfoibVrjVdnda9vG2ut1Q/640?wx_fmt=png&from=appmsg)

这次，我们不直接匹配图片和名字，而是把宝可梦识别拆成几个部分，就像玩“宝可梦连连看”一样。

我们定义了五组关键属性：主要颜色（比如“白色”、“蓝色”）、主要形态（比如“一只狼”、“一只有翅膀的爬行动物”）、关键特征（比如“一只白色的角”、“大翅膀”）、体型（比如“四脚着地的狼形”、“有翅膀且纤细”）以及背景场景（比如“外太空”、“绿色森林”）。

**对于每一组属性，我们都设计了一个专门的提示词，比如“这只宝可梦的身体主要是{}色的”，然后把可能的选项填进去。**接着，我们用模型计算图片和每个选项的相似度得分，再用 softmax 函数把得分转换成概率，这样就能更好地衡量模型的置信度。

**完整的思维链（CoT）由两部分组成：**`classification_groups` 和 `pokemon_rules`，前者定义了提问框架：每个属性（例如颜色、形态）对应一个问题模板和一系列可能的答案选项。后者则记录了每只宝可梦应该匹配哪些选项。

例如，Absol 的颜色应该是“白色”，形态应该是“狼形”。我们后面会讲怎么构建完整的 CoT 结构，下面的 pokemon_system（精灵宝可梦系统）是 CoT 的一个具体实例：

```
pokemon_system = {    "classification_cot": {        "dominant_color": {            "prompt": "This Pokémon's body is mainly {} in color.",            "options": [                "white",    # Absol, Absol G                "gray",     # Aggron                "brown",    # Aerodactyl, Weedle, Beedrill δ                "blue",     # Azumarill                "green",    # Bulbasaur, Venusaur, Celebi&Venu, Caterpie                "yellow",   # Alakazam, Ampharos                "red",      # Blaine's Moltres                "orange",   # Arcanine                "light blue"# Dratini            ]        },        "primary_form": {            "prompt": "It looks like {}.",            "options": [                "a wolf",         # Absol, Absol G                "an armored dinosaur",  # Aggron                "a winged reptile",     # Aerodactyl                "a rabbit-like creature", # Azumarill                "a toad-like creature",   # Bulbasaur, Venusaur, Celebi&Venu                "a caterpillar larva",    # Weedle, Caterpie                "a wasp-like insect",     # Beedrill δ                "a fox-like humanoid",     # Alakazam                "a sheep-like biped",      # Ampharos                "a dog-like beast",        # Arcanine                "a flaming bird",          # Blaine's Moltres                "a serpentine dragon"      # Dratini            ]        },        "key_trait": {            "prompt": "Its most notable feature is {}.",            "options": [                "a single white horn", # Absol, Absol G                "metal armor plates",  # Aggron                "large wings",         # Aerodactyl, Beedrill δ                "rabbit ears",         # Azumarill                "a green plant bulb",  # Bulbasaur, Venusaur, Celebi&Venu                "a small red spike",   # Weedle                "big green eyes",      # Caterpie                "a mustache and spoons", # Alakazam                "a glowing tail orb",  # Ampharos                "a fiery mane",        # Arcanine                "flaming wings",       # Blaine's Moltres                "a tiny white horn on head" # Dratini            ]        },        "body_shape": {            "prompt": "The body shape can be described as {}.",            "options": [                "wolf-like on four legs",   # Absol, Absol G                "bulky and armored",        # Aggron                "winged and slender",       # Aerodactyl, Beedrill δ                "round and plump",          # Azumarill                "sturdy and four-legged",   # Bulbasaur, Venusaur, Celebi&Venu                "long and worm-like",       # Weedle, Caterpie                "upright and humanoid",     # Alakazam, Ampharos                "furry and canine",         # Arcanine                "bird-like with flames",    # Blaine's Moltres                "serpentine"                # Dratini            ]        },        "background_scene": {            "prompt": "The background looks like {}.",            "options": [                "outer space",      # Absol G, Beedrill δ                "green forest",     # Azumarill, Bulbasaur, Venusaur, Weedle, Caterpie, Celebi&Venu                "a rocky battlefield", # Absol, Aggron, Aerodactyl                "a purple psychic room", # Alakazam                "a sunny field",     # Ampharos                "volcanic ground",   # Arcanine                "a red sky with embers", # Blaine's Moltres                "a calm blue lake"   # Dratini            ]        }    },    "pokemon_rules": {        "Absol": {            "dominant_color": 0,            "primary_form": 0,            "key_trait": 0,            "body_shape": 0,            "background_scene": 2        },        "Absol G": {            "dominant_color": 0,            "primary_form": 0,            "key_trait": 0,            "body_shape": 0,            "background_scene": 0        },        // ...    }}
```

总之，现在我们不是简单地比较一次相似度，而是进行多次比较，把各个属性的概率综合起来，这样就能做出更合理的判断。

```
# 分类流程def classify_pokemon(image):   # 生成所有提示   all_prompts = []   for group in classification_cot:       for option in group["options"]:           prompt = group["prompt"].format(option)           all_prompts.append(prompt)   # 获取向量及其相似度   image_embedding = model.encode_image(image)   text_embeddings = model.encode_text(all_prompts)   similarities = cosine_similarity(image_embedding, text_embeddings)   # 将相似度转换为每个属性组的概率   probabilities = {}   for group_name, group_sims in group_similarities:       probabilities[group_name] = softmax(group_sims)   # 根据匹配的属性计算每个宝可梦的得分   scores = {}   for pokemon, rules in pokemon_rules.items():       score = 0       for group, target_idx in rules.items():           score += probabilities[group][target_idx]       scores[pokemon] = score   return max(scores, key=scores.get) # 返回得分最高的宝可梦
```

#### 两种方法的复杂度分析

现在我们来分析一下复杂度，假设我们要在 N 个宝可梦名字中找到与给定图片最匹配的名字：

基准方法需要计算 N 个文本向量（每个名字对应一个）以及 1 个图片向量，然后进行 N 次相似度计算（图片向量与每个文本向量比较）。**因此，基准方法的复杂度主要取决于文本向量的计算次数 N。**

而我们的 CoT 方法需要计算 Q 个文本向量，其中 Q 是所有问题和选项的组合总数，以及 1 个图片向量。之后，需要进行 Q 次相似度计算（图片向量与每个问题-选项组合的文本向量比较）。**因此，该方法的复杂度主要取决于 Q。**

在这个例子中，N = 13，Q = 52（5 组属性，平均每组约 10 个选项）。两种方法都需要计算图像向量并执行分类步骤，在比较中我们就舍去了这些共同操作。

极端情况下，如果 Q = N，那我们的方法实际上就退化成基准方法了。所以，想要有效地拓展推理时计算，关键在于：

*   设计好问题，增加 Q 的值。
    
*   确保每个问题都能提供有用的线索，帮我们缩小范围。
    
*   问题之间最好不要有重复信息，这样才能最大化信息增益。
    

就像玩“二十个问题”一样，每个问题都要精心设计，才能有效缩小可能的答案范围，快速猜到答案。

#### 实验结果

我们在 117 张测试图片上进行了评估，包含 13 种不同的宝可梦。准确率结果如下：

![Image 25](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geia3AjM9QlnqibtWbBnUwuep9MeMEHWWPWgYkPiaOaqJGf1mfGNWWyHaKjw/640?wx_fmt=png&from=appmsg)

可以看到，同样的 CoT 分类方法在这类不常见的、OOD 的数据上，对两个模型都有明显的提升（分别提升了 15.25% 和 22.04%）。

这也说明，一旦`pokemon_system`构建好了，**同一个 CoT，可以不改代码直接用在不同的模型上，而且不需要微调或额外的训练。**

有意思的是，`jina-clip-v1`模型在宝可梦分类上的基础准确率就比较高（31.36%），因为它是在包含宝可梦数据的 LAION-400M 数据集上训练的。而 `jina-clip-v2`模型是在 DFN-2B 上训练的，这个数据集质量更高，但也过滤掉了更多数据，可能把宝可梦相关的内容也去掉了，所以它的基础准确率比较低（16.10%）。

#### 等下，这个方法是怎么 work 的？

👩‍🏫 **让我们回顾一下我们做了什么**

我们一开始使用的是固定的预训练向量模型，这些模型无法处理零样本的分布外（OOD）问题。但当我们建立了一个分类树后，它们突然就可以做到了。这其中的秘诀是什么呢？是不是像传统机器学习中的弱学习器集成的思路？

值得注意的是，我们的向量模型能够从"摆烂"升级到"支棱"，并不是因为集成学习本身，而是因为分类树中包含的外部领域知识。你可以对成千上万个问题反复进行零样本分类，但如果这些答案对最终结果没有帮助，那就毫无意义。这就像"你说我猜"（二十个问题）游戏，你需要通过每个问题逐步缩小解决方案的范围。

**因此，这种外部知识或思维过程才是关键** **- 就像我们的例子中，关键在于精灵宝可梦系统是如何构建的。**这种专业知识可以来自人类，也可以来自大语言模型。

#### 高效构建思维链系统

**推理时计算的效果好坏，很大程度上取决于****`pokemon_system`****的质量****。**构建这个 CoT 系统的方法有很多，从手动到全自动，各有优劣。

##### 1. 手动构建

这是最直接的办法，手动分析宝可梦数据集，创建属性组、提示和规则。

领域专家需要找出关键的视觉属性，比如颜色、形态、特征等等。然后，为每个属性写一个自然语言提示词，列出所有可能的选项，再把每个宝可梦和对应的属性选项关联起来。

**这种方法的规则质****量高，精准贴合数据集特点；但是太费时费力，而且数据量一大就不好弄了。**

##### 2. LLM 辅助构建

也可以用 LLM 来生成分类系统，我们需要给 LLM 一个清晰的提示词，需要包含以下信息：基于视觉特征的属性组、提示词模板、全面且互斥的所有可能的选项，以及每个宝可梦对应的规则。

```
我需要一个宝可梦分类系统。对于以下宝可梦：[Absol, Aerodactyl, Weedle, Caterpie, Azumarill, ...]，创建一个包含以下内容的分类系统：1. 基于以下视觉属性的分类组：   - 宝可梦的主要颜色   - 宝可梦的形态   - 宝可梦最显著的特征   - 宝可梦的整体体型   - 宝可梦通常出现的背景环境2. 对于每个分类组：   - 创建一个自然语言提示模板，用 "{}" 表示选项   - 列出所有可能的选项   - 确保选项互斥且全面3. 创建规则，将每个宝可梦映射到每个属性组中的一个选项，使用索引引用选项请以 Python 字典格式输出，包含两个主要部分：- "classification_groups": 包含每个属性的提示和选项- "pokemon_rules": 将每个宝可梦映射到其对应的属性索引示例格式：{    "classification_groups": {        "dominant_color": {            "prompt": "This Pokemon's body is mainly {} in color.",            "options": ["white", "gray", ...]        },        ...    },    "pokemon_rules": {        "Absol": {            "dominant_color": 0,  # "white" 的索引            ...        },        ...    }}
```

LLM 很快能生成一个初稿，但也需要人工检查和修正。

更靠谱的办法是 **结合 LLM 生成和人工验证**。先让 LLM 生成一个初始版本，然后人工检查和修改属性分组、选项和规则，再把修改意见反馈给 LLM，让它继续完善，直到满意为止。这种方法兼顾了效率和准确性。

##### 3. 用 DSPy 自动化构建

对于全自动构建 `pokemon_system`，可以用 DSPy 迭代优化。

先从一个简单的 `pokemon_system` 开始，可以是手动创建的，也可以是 LLM 生成的。然后用留出集的数据评估它的效果，把准确率作为反馈信号给 DSPy。DSPy 会根据这个反馈生成新的 `pokemon_system`，不断重复这个循环，直到性能收敛，不再有明显提升为止。

**整个过程中，向量模型都是固定不变的**。通过 DSPy 就能自动找到最佳的 pokemon_system（CoT）设计，且每个任务只需要调优一次。

![Image 26](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geia7g95zY8KkOKoKzHFbFo5CW4eXR2UPtuvricPPjvcszZb9dUz5ld8CJQ/640?wx_fmt=png&from=appmsg)

为什么要在向量模型上 Scaling Test-Time Compute？

因为一直加大预训练模型的规模，成本太高，扛不住啊。

Jina Embeddings 系列，从`jina-embeddings-v1`、`v2`、`v3` 到 `jina-clip-v1`、`v2`，还有 `jina-ColBERT-v1`、`v2`，每次升级都是靠更大的模型、更多的预训练数据，成本也越来越高。

就拿`jina-embeddings-v1`来说，2023 年 6 月发布的时候，1.1 亿参数，训练成本就要 5000 到 10000 美元。到了 `jina-embeddings-v3`，性能提升了不少，但主要还是靠砸钱堆资源。现在，顶级模型的训练成本已经从几千美元涨到了几万美元，大公司甚至要花几亿美元。虽然预训练投入越多，模型效果越好，但成本太高，性价比越来越低，发展终究需要考虑可持续性。

![Image 27](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3IjMoyELZt7ZKRTgsuY0geiaHPGSDXkjapMy2mYaC99geJrcdqoJicqkxI86SQOzeX5BVkYpOZ1HIqg/640?wx_fmt=png&from=appmsg)

向量模型的 Scaling Law

**这张图就展示了向量模型的 Sc****aling Law。**横轴是模型参数量，纵轴是 MTEB 的平均性能。每个点代表一个向量模型。趋势线代表所有模型的平均水平，蓝色的点是多语言模型。

这些数据选自 MTEB 排行榜排名前 100 的向量模型。为了保证数据质量，我们过滤掉了未公开模型大小信息的模型以及一些无效的提交。

另一方面，现在的向量模型已经很强大了：多语言、多任务、多模态，零样本学习和指令跟随能力都很出色。**这种多功能性为算法改进和扩展推理时计算带来了巨大的想象空间。**

关键问题是：**对于一个用户真正关心的查询，他们愿意付出多少代价**？如果仅仅是让固定的预训练模型的推理时间稍长一些，就能大幅提升结果质量，相信很多人都会觉得物超所值。

在我们看来，**扩展推理时计算在向量模型领域蕴藏着巨大的有待挖掘的潜力**，这或许将是未来研究的一个重要突破口。**与其一味追求更大的模型，不如在推理阶段多下功夫，探索更巧妙的计算方法来提升性能** —— 这可能是一条更经济、也更有效的路径。

### 结论

在 `jina-clip-v1/v2` 的实验表现里，我们观察到以下几个关键现象：

1.  我们 **在模型没见过的、领域外（OOD）的数据上****，**取得了更好的识别准确率，并且完全没有对模型进行任何微调或额外的训练。
    
2.  该系统通过 **迭代地细化相似性搜索和分类标准**，实现了更精细的区分能力。
    
3.  通过引入 **动态提示调整和迭代推理**（类似于“思维链”），我们将向量模型的推理流程从单一查询转变为更复杂的思维链。
    

**这仅仅是开始！Scaling Test-Time Compute 的潜力远不止于此**，还有广阔空间值得我们去探索。比如，我们可以开发出更高效的算法，通过迭代选择最有效的策略，来缩小答案空间，类似于‘二十个问题’游戏中最优解法的策略。通过拓展推理时计算，我们可以推动向量模型突破现有瓶颈，解锁曾经看似遥不可及的复杂精细的任务，将这些模型推向更广阔的应用前景。

![Image 28](https://mmbiz.qpic.cn/sz_mmbiz_png/3EjVEqjYF3KkbW7JOgtWjS0Vc3iavuoNLX28NiaU5Ze5CLfxJfDtC5uxvvS3q4suqBdCOWj4ia0J7CznCRPD4Dj2Q/640?wx_fmt=png)
